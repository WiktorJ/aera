# FROM nvidia/cuda:12.9.0-cudnn-runtime-ubuntu24.04
FROM nvidia/cuda:12.4.0-devel-ubuntu22.04
# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV XLA_PYTHON_CLIENT_MEM_FRACTION=0.9
ENV XDG_CACHE_HOME="/workspace/.cache"
ENV OPENPI_DATA_HOME="/workspace/.cache/openpi"
ENV AERA_CHECKPOINT_DIR="/workspace/checkpoints"
ENV MLFLOW_TRACKING_URI="http://127.0.0.1:5000"

# RunPod mounts the GPU driver files here. We must add this to the path
# so JAX finds the real hardware driver (libcuda.so) before the venv libs.
ENV LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:$LD_LIBRARY_PATH


# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-dev \
    python3-pip \
    git \
    curl \
    vim \
    build-essential \
    openssh-server \
    && rm -rf /var/lib/apt/lists/*

# Install uv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

# Copy project files
COPY . .

# Install the project
RUN --mount=type=cache,target=/root/.cache/uv uv sync --python 3.12

# Make the training script executable
RUN chmod +x aera/autonomous/openpi/scripts/run_runpod_train.sh
RUN echo "cd /app" >> /root/.bashrc

# Setup start script
COPY docker/start.sh /start.sh
RUN chmod +x /start.sh

# Set the entrypoint to our script
ENTRYPOINT ["/start.sh"]

